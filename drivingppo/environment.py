"""
PPO ëª¨ë¸ ì„¤ì •ê°’ë“¤, í›ˆë ¨ í™˜ê²½ê³¼ í›ˆë ¨ í•¨ìˆ˜ë“¤, ê°ì¢… ë³€í™˜ í•¨ìˆ˜, ìœ í‹¸ ë“±.
"""
from typing import Callable, Literal
import math

from .world import World, distance_of, angle_of, pi, pi2, rad_to_deg
from .simsim import WorldViewer
from .common import (
    SPD_MAX_STD,
    LIDAR_START,
    LIDAR_END,
    LIDAR_NUM,
    LIDAR_RANGE,
    LOOKAHEAD_POINTS,
    OBSERVATION_IND_SPD,
    OBSERVATION_IND_GOAL_0,
    OBSERVATION_IND_GOAL_1,
    OBSERVATION_IND_LIDAR_DIS_S,
    OBSERVATION_IND_LIDAR_DIS_E,
    OBSERVATION_DIM,
)

import numpy as np
from numpy import ndarray as Arr
import gymnasium as gym
from gymnasium import spaces


def get_state(world:World):
    """
    Worldì˜ í˜„ì¬ ìƒíƒœë¥¼ RL ì…ë ¥ ë²¡í„°(ê³ ì • í¬ê¸°)ë¡œ ë³€í™˜
    """
    p = world.player
    s_norm = speed_norm(p.speed)

    # ê²½ë¡œ ì •ë³´
    path_data = get_path_features(world)

    # ë¼ì´ë‹¤ ê±°ë¦¬ê°€ê¹Œìš´ì ìˆ˜
    obs_near = [distance_score_near(distance) if h else 0.0
                for _,_, distance, _,_,_, h in world.lidar_points]

    # ëª¨ë“  ë²¡í„°ë¥¼ í•©ì³ ê³ ì •ëœ í¬ê¸°ì˜ ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    observation = np.array([s_norm] + path_data + obs_near, dtype=np.float32)

    return observation

def speed_norm(speed):
    return min(speed / SPD_MAX_STD, 1.0)  # ê°€ëŠ¥í•œ ìµœëŒ€ ì†ë ¥ì€ 19ì¯¤ì´ì§€ë§Œ ì‹¤ì œë¡œ 7ì´ ë„˜ì–´ê°€ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì–´ì„œ ìµœëŒ€ ì†ë ¥ 10ìœ¼ë¡œ ì¹˜ê³  ì •ê·œí™”.

def get_path_features(world:World) -> list[float]:
    """
    ê²½ë¡œ ì •ë³´
    ë°”ë¡œ ì•ì˜ ì  ëª‡ ê°œì˜ ê±°ë¦¬ì™€ ê°ë„.
    """

    path_data = []
    x0 = world.player.x
    z0 = world.player.z
    a0 = world.player.angle_x

    # ê° ëª©í‘œì ì˜ ê±°ë¦¬, ê°ë„ ì •ë³´
    for index in range(
            world.current_goal_idx,
            world.current_goal_idx + LOOKAHEAD_POINTS
        ):
        # ì´ì „ ëª©í‘œì  ê¸°ì¤€
        if index < world.path_len:
            x1, z1 = world.goal_points[index]
            d_from_prev = distance_of(x0, z0, x1, z1)
            a1          = angle_of(x0, z0, x1, z1)
            a_from_prev = a1 - a0
            x0 = x1
            z0 = z1
            a0 = a1
        else:
            d_from_prev = 0.0
            a_from_prev = 0.0

        # ì—ì´ì „íŠ¸ ê¸°ì¤€
        # d_from_agnt = world.get_distance_to_goal(index)
        a_from_agnt = world.get_relative_angle_to_goal(index)

        a_fp_norm = ((a_from_prev + pi) % pi2 - pi) / pi  # ê°ë„(ì´ì „ ëª©í‘œì  ê¸°ì¤€)
        a_fa_norm = ((a_from_agnt + pi) % pi2 - pi) / pi  # ê°ë„(ì—ì´ì „íŠ¸ ê¸°ì¤€)
        d_near = distance_score_near(d_from_prev)  # ê±°ë¦¬ ê°€ê¹Œìš´ ì •ë„
        d_far  = distance_score_far(d_from_prev)   # ê±°ë¦¬ ë¨¼ ì •ë„

        path_data.extend([a_fp_norm, a_fa_norm, math.cos(a_fa_norm), d_near, d_far])

    return path_data

def observation_str(observation):
    agent_speed     = observation[OBSERVATION_IND_SPD]
    obs_goal_afp_0  = observation[OBSERVATION_IND_GOAL_0]
    obs_goal_afa_0  = observation[OBSERVATION_IND_GOAL_0 +1]
    obs_goal_dist_0 = observation[OBSERVATION_IND_GOAL_0 +4]
    obs_goal_afp_1  = observation[OBSERVATION_IND_GOAL_1]
    obs_goal_afa_1  = observation[OBSERVATION_IND_GOAL_1 +1]
    obs_goal_dist_1 = observation[OBSERVATION_IND_GOAL_1 +4]
    obs_goal_afp_2  = observation[OBSERVATION_IND_GOAL_1 + 4]
    obs_goal_afa_2  = observation[OBSERVATION_IND_GOAL_1 + 4 +1]
    obs_goal_dist_2 = observation[OBSERVATION_IND_GOAL_1 + 4 +4]
    return f'STATE:  speed {agent_speed:+.2f}({speed_norm(agent_speed):+.2f})'\
           f' | Path'\
           f' [0] a:{obs_goal_afp_0*pi*rad_to_deg:+5.2f}({obs_goal_afa_0*pi*rad_to_deg:+.2f}) d:{obs_goal_dist_0:.2f}'\
           f' [1] a:{obs_goal_afp_1*pi*rad_to_deg:+5.2f}({obs_goal_afa_1*pi*rad_to_deg:+.2f}) d:{obs_goal_dist_1:.2f}'\
           f' [2] a:{obs_goal_afp_2*pi*rad_to_deg:+5.2f}({obs_goal_afa_2*pi*rad_to_deg:+.2f}) d:{obs_goal_dist_2:.2f}'

def _distance_score_near(x:float) -> float:
    d = x + 10.0
    x = 100./d/d
    if x <= 1:
        return x
    else:
        return 1.0

distance_score_near_base = _distance_score_near(LIDAR_RANGE)

def distance_score_near(x:float) -> float:
    return max(0, _distance_score_near(x) - distance_score_near_base)

def distance_score_far(distance:float) -> float:
    return math.log(distance + 1.0)/10.0


def apply_action(world:World, action:Arr):
    """
    í–‰ë™ ë²¡í„° [A_forward, A_steer]ë¥¼ Worldì˜ ì œì–´ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©
    """

    A_forward, A_steer = action

    # WS
    if A_forward > 0:
        world.moveWS('W', A_forward)
    else:
        world.moveWS('S', -A_forward)

    # AD
    if A_steer > 0: # ì–‘ìˆ˜: ìš°íšŒì „ (D)
        world.moveAD('D', A_steer)
    else: # ìŒìˆ˜: ì¢ŒíšŒì „ (A)
        world.moveAD('A', -A_steer)

def action_str(action):
    return f'ACTION: {action[0]:.2f}  {action[1]:.2f}'



class WorldEnv(gym.Env):
    """
    Worldì—ì„œ ì£¼í–‰ë²•ì„ ê°•í™”í•™ìŠµí•˜ê¸° ìœ„í•œ gym í™˜ê²½ í´ë˜ìŠ¤.
    """

    time_gain_per_goal_point = 10_000
    time_gain_limit = 20_000

    def __init__(self,
                 world_generator:Callable[[], World],
                 max_time=120_000,
                 time_step=111,
                 step_per_control=3,
                 render_mode:Literal['window','debug']|None=None,
                 auto_close_at_end=True):

        super().__init__()
        self.closed = False

        self.lidar_angles = np.linspace(LIDAR_START, LIDAR_END, LIDAR_NUM)

        self.time_step = time_step
        self.step_per_control = step_per_control  # ì¡°ì‘ê°’ ë³€ê²½ì€ ì›”ë“œì˜ nìŠ¤í…ë§ˆë‹¤ í•œ ë²ˆ. Tank Challengeì—ì„œë„ FPSëŠ” 30ì´ì–´ë„ API ìš”ì²­ì€ ìµœì†Œ 0.1ì´ˆë§ˆë‹¤ í•œ ë²ˆìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥í•˜ë‹¤.
        self.max_time = max_time  # ìµœëŒ€ ì—í”¼ì†Œë“œ íƒ€ì„

        # Action: [A_forward, A_steer]
        self.action_space = spaces.Box(  # Forward, Steer
            low=np.array([-1.0, -1.0], dtype=np.float32),
            high=np.array([1.0, 1.0], dtype=np.float32),
            dtype=np.float32
        )

        # Observation Space ì •ì˜ (ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(OBSERVATION_DIM,),
            dtype=np.float32
        )

        self.world_generator = world_generator

        """
        render_mode
        None: ì¡°ìš©íˆ
        'window': ì°½ ë„ì›€
        'debug': ì°½ + í„°ë¯¸ë„ì— í…ìŠ¤íŠ¸
        """
        self.render_mode = render_mode
        self.auto_close_at_end = auto_close_at_end
        self.viewer:WorldViewer|None = None
        print(f'WorldEnv render:{self.render_mode}')


    @property
    def observation(self):
        return get_state(self.world)


    def step(self, action):
        """
        í–‰ë™ì„ ì‹¤í–‰í•˜ê³ , ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ë¥¼ ë°˜í™˜
        """
        observation0 = self.observation

        if self.closed:  # ì°½ ë‹«ì•„ì„œ ì¢…ë£Œ
            return observation0, 0, False, True, {}

        self.step_count += 1
        if self.render_mode == 'debug': print(f'{self.step_count} step --------------------------')
        if self.render_mode == 'debug': print(observation_str(observation0))

        w = self.world
        p = w.player
        l = w.lidar

        apply_action(self.world, action)
        result_collision = False
        result_goal = False
        for _ in range(self.step_per_control):
            _, result_collision_step, result_goal_step = w.step(self.time_step)
            result_collision += result_collision_step
            result_goal      += result_goal_step
        if self.step_count == 1:
            if result_collision: print(f'ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ ë§µ í™•ì¸ í•„ìš”: ì‹œì‘ê³¼ë™ì‹œì— ì¶©ëŒ (hint: ëª©í‘œì  ìˆ˜ {w.path_len})')
            if result_goal:      print(f'ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ğŸ’¥ ë§µ í™•ì¸ í•„ìš”: ì‹œì‘ê³¼ë™ì‹œì— ê³¨ (hint: ëª©í‘œì  ìˆ˜ {w.path_len})')

        observation1 = self.observation

        if self.render_mode == 'debug': print(action_str(action))

        terminated = False
        truncated = False
        ending = ''

        s_norm = speed_norm(p.speed)  # ì†ë„ì ìˆ˜
        distance = w.get_distance_to_goal() +1
        cos_a = math.cos(w.get_relative_angle_to_goal())
        cos_a1 = math.cos(w.get_relative_angle_to_goal(1))

        obs0 = observation0[OBSERVATION_IND_LIDAR_DIS_S:OBSERVATION_IND_LIDAR_DIS_E].max()
        obs1 = observation1[OBSERVATION_IND_LIDAR_DIS_S:OBSERVATION_IND_LIDAR_DIS_E].max()
        obs_d = obs1 - obs0

        reward_step = [0.0 for _ in range(7)]

        # ì¶©ëŒ
        if result_collision:
            reward_step[2] += -200.0
            ending = 'ì¶©ëŒ'
            terminated = True

        # ëª©í‘œì  ë„ë‹¬
        elif result_goal:
            reward_step[1] += 20.0 + 20.0 * cos_a1
            if self.render_mode == 'debug': print(f'â˜… {reward_step[1]:.1f} ~ {int(round(w.get_relative_angle_to_goal(1))*rad_to_deg)}({cos_a1:.2f})')

            # ì¶”ê°€ì‹œê°„ íšë“; ê·¸ëŸ¬ë‚˜ ë¬´í•œì • ìŒ“ì´ì§€ëŠ” ì•ŠìŒ.
            self.time_limit += self.time_gain_per_goal_point
            self.time_limit = min(self.time_limit, w.t_acc + self.time_gain_limit)

            # ìµœì¢… ëª©í‘œ ë„ë‹¬
            if w.arrived:
                ending = 'ë„ì°©'
                reward_step[1] += -30.0 * s_norm  # ë„ì°©ì‹œ ì •ì§€
                terminated = True

        # ì „í˜€ ì—‰ëš±í•œ ê³³ ê°
        elif distance > w.far:
            reward_step[2] += 100.0 * p.speed / SPD_MAX_STD * cos_a
            if self.render_mode == 'debug': print(f'LOST ({distance:.1f} > {w.far:.1f}) reward: {reward_step[2]:.2f}')
            ending = 'ê¸¸ìƒìŒ'
            truncated = True

        # ì‹œê°„ ë‚´ì— ë„ì°© ëª» í•¨
        elif w.t_acc >= self.time_limit:
            reward_step[2] += -200.0  # ëª©ì ì§€ê°€ ì½”ì•ì¸ë° ë²½ì•ì—ì„œ ê°€ë§Œíˆìˆê¸°ë¥¼ íƒí•˜ì§€ ì•Šë„ë¡ ì¶©ëŒë§Œí¼ì˜ ë²Œì . ëŒ€ì‹  ì‹œê°„ì€ ë„‰ë„‰íˆ ì¤Œ.
            ending = 'ì‹œê°„ì´ˆê³¼'
            terminated = True

        # íšë“í•œ ì‹œê°„ì€ ëª¨ìë¥´ì§€ ì•Šìœ¼ë‚˜ ê·¸ëƒ¥ ì´ì œê¹Œì§€ ë§ì´ í•¨.
        elif w.t_acc >= self.max_time:
            ending = 'ì‹œê°„í•œê³„'
            truncated = True

        if truncated or terminated:
            icon = \
                'âœ…' if ending == 'ë„ì°©' else \
                'â–¶ï¸' if ending == 'ì‹œê°„í•œê³„' else \
                'ğŸ’¥' if ending == 'ì¶©ëŒ' else \
                'ğŸ‘»' if ending == 'ê¸¸ìƒìŒ' else \
                'â°' if ending == 'ì‹œê°„ì´ˆê³¼' else '??'
            print(f'ê²°ê³¼{icon} ë„ì°©: {w.current_goal_idx:3d}/{w.path_len:3d} | ì‹œê°„: {int(w.t_acc/1000):3d}/{int(self.time_limit/1000):3d}/{int(self.max_time/1000):3d} ì´ˆ ({int(w.t_acc/self.max_time*100):3d}%) | ìœ„ì¹˜: {int(p.x):4d}, {int(p.z):4d} ({int(p.x/self.world.MAP_W*100):3d}%, {int(p.z/self.world.MAP_H*100):3d}%)')

        else:
            # ì§„í–‰ ë³´ìƒ

            reward_time = -0.15

            stat_progress     = + (cos_a * s_norm) * 0.3  if s_norm > 0 \
                           else - s_norm * s_norm * 0.6  # í›„ì§„ ì§„í–‰ ì–µì œ
            stat_orientation  = + cos_a * 0.06
            danger            = - obs1 * 0.12
            danger_d          = - obs_d * 8.0
            if self.render_mode == 'debug': print(f'REWARD: time {reward_time:.1f} |  prog {stat_progress:.2f} | ang {stat_orientation:.2f} | danger {danger:.2f} ~  {danger_d:.2f}')

            reward_step[2] += self.step_per_control * reward_time
            reward_step[3] += self.step_per_control * stat_progress
            reward_step[4] += self.step_per_control * stat_orientation
            reward_step[5] += self.step_per_control * danger
            reward_step[6] += self.step_per_control * danger_d

        info = {'current_time': w.t_acc / 1000.0}

        # ì ìˆ˜ í•©
        reward_step[0] = sum(reward_step[1:])
        for i in range(7):
            self.reward_totals[i] += reward_step[i]
        if truncated or terminated:
            self.print_result()

        # Gymnasium í‘œì¤€ ë°˜í™˜
        return observation1, reward_step[0], terminated, truncated, info


    def reset(self, *, seed=None, options=None):
        """
        í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ë°˜í™˜
        """
        super().reset(seed=seed)

        self.reset_randomly()

        self.step_count = 0
        self.reward_totals = [0.0 for _ in range(7)]
        self.time_limit = self.time_gain_limit  # ì œí•œì‹œê°„. ëª©í‘œì  ë„ë‹¬ì‹œë§ˆë‹¤ ì¶”ê°€ íšë“.

        w = self.world
        p = w.player
        self.S_MAX = p.speed_max_w(1)  # ìµœëŒ€ì†ë„

        observation = self.observation
        info = {}
        return observation, info

    def reset_randomly(self):
        self.world = self.world_generator()


    def render(self):
        if self.render_mode == None: return
        if self.closed:
            self.close()
            return

        # ì§€ì—° ì´ˆê¸°í™”: WorldViewerê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìƒì„±í•©ë‹ˆë‹¤.
        if self.viewer is None:
            self.viewer = WorldViewer(self.world, auto_update=False)
        elif self.viewer.world is not self.world:
            self.viewer.close()
            self.viewer = WorldViewer(self.world, auto_update=False)

        if self.viewer.closed: self.closed = True; return

        self.viewer.update()

    def print_result(self):
        print(f'ì´ì  {int(self.reward_totals[0]):5d} | goal {self.reward_totals[1]:6.1f} | time {self.reward_totals[2]:+7.2f} | prog {self.reward_totals[3]:+7.2f} | ang {self.reward_totals[4]:+7.2f} | danger {self.reward_totals[5]:+7.2f} ~ {self.reward_totals[6]:+7.2f}')


    def close(self):
        self.print_result()
        self.closed = True
        if self.viewer is None: return
        if self.auto_close_at_end:
            self.viewer.close()
            self.viewer = None
        else:
            self.viewer.mainloop()
            self.viewer = None
        print('WorldEnv closed')
